{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, Image, display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@368.704] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading video file\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "display_handle=display(None, display_id=True)\n",
    "\n",
    "if video.isOpened() == False:\n",
    "        print(\"Error reading video file\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    _, frame = cv2.imencode('.jpeg', frame)\n",
    "    display_handle.update(Image(data=frame.tobytes()))\n",
    "    \n",
    "    # result = face_detector.detect_emotions(frame)\n",
    "\n",
    "    # for index, person in enumerate(result):\n",
    "\n",
    "        # if not stats.__contains__(index):\n",
    "            # stats[index] = []\n",
    "        # stats[index].append(person['emotions'])\n",
    "        # bounding_box = person['box']\n",
    "        # cv2.rectangle(frame, (bounding_box[0], bounding_box[1]), (bounding_box[0] + bounding_box[2], bounding_box[1] + bounding_box[3]), (255, 0, 0), 4)\n",
    "\n",
    "        # max_value = 0\n",
    "        # for value in person['emotions'].values():\n",
    "            # if value > max_value:\n",
    "                # max_value = value\n",
    "\n",
    "        # cv2.putText(frame, f\"Person {index}\", (bounding_box[0], bounding_box[1] - TEXT_SPACING), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        # for i, (emotion, value) in enumerate(person['emotions'].items()):\n",
    "            # if value == max_value:\n",
    "                # color = (0, 255, 0)\n",
    "            # else:\n",
    "                # color = (0, 0, 255)\n",
    "            # cv2.putText(frame, f\"{emotion}: {value}\", (bounding_box[0], bounding_box[1] + bounding_box[3] + TEXT_SPACING * (i + 1)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "    if ret == True:\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "\n",
    "        # cv2.imshow('Press q to close', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61468da9f15d03f379c04cf0dd5a78cf227ce594ced04d10acd5ad9790574dd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
